{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59da3107-5f9b-4253-af29-ca584de8e633",
   "metadata": {},
   "source": [
    "Importing all libraries and functions used in the Thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e471438e-6d52-4268-9168-a923dd071f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsfresh\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, make_scorer, roc_auc_score,\n",
    "matthews_corrcoef,recall_score \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from tsfresh import extract_features, extract_relevant_features, select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters, MinimalFCParameters\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import lightgbm as lgbm\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b299fd69-a15b-473e-8ad8-3adf5b15134d",
   "metadata": {},
   "source": [
    "AUs were extracted from OpenFace 2.0 software and CSV files with time series data were created. \n",
    "Each csv files corresponds to one video of donor at particular stage in donation process. \n",
    "Now, imputation of all the rows in csv file which has confidence lower than 0.85 is conducted using KNN Imputer.\n",
    "This is crucial to ensure that no noise is present in the dataset with values of low confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d828e1d6-7305-4a07-ab4c-2a0cb8f9ebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_path = \"confidential\"\n",
    "\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        id_num = re.search(r'video(\\d+)_', filename).group(1)\n",
    "        df = pd.read_csv(os.path.join(folder_path, filename))\n",
    "\n",
    "        if len(df.columns) < 4:\n",
    "            continue\n",
    "        # Add ID and Frame columns which is needed for identification\n",
    "        df['ID'] = id_num\n",
    "        df['Frame'] = df.index + 1\n",
    "\n",
    "        cols_to_impute = [col for col in df.columns if col.endswith('_r') or col == df.columns[3]]\n",
    "        conf_mask = df.iloc[:, 3] < 0.85\n",
    "        if conf_mask.all():\n",
    "            print(f\"Skipping file {filename} as all columns have less than 85% confidence\")\n",
    "            continue\n",
    "        df.loc[conf_mask, cols_to_impute] = pd.np.nan\n",
    "        \n",
    "        imputer = KNNImputer(n_neighbors = 7)\n",
    "        imputer.fit(df)\n",
    "        \n",
    "        imputed_values = pd.DataFrame(imputer.fit_transform(df[cols_to_impute]), columns = cols_to_impute)\n",
    "        imputed_values = imputed_values.drop(' confidence', axis=1)\n",
    "        imputed_values[\"ID\"] = id_num\n",
    "        imputed_values[\"Frame\"] = df.index + 1\n",
    "\n",
    "        # Saving new files\n",
    "        new_filename = filename.replace(\".csv\", \"_final_imputation.csv\")\n",
    "        imputed_values.to_csv(os.path.join(folder_path, new_filename), index=False)\n",
    "###THIS CODE HAD TO DROP 186_6.csv and 190_6.csv files because they had no rows with confidence above 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490ddf7e-cb59-4f65-a967-f2f2ebf92fc7",
   "metadata": {},
   "source": [
    "PLEASE NOTE THAT FROM HERE ON, PROCESS LOOKS EXATCLY THE SAME FOR TIME POINTS 4, 5 AND 6. THEREFORE, ONLY PROCESS FOR TIME POINT 4 IS INCLUDED IN THIS NOTEBOOK AS THE OTHERS CAN BE REPLICATED BY FOLLOING THESE STEPS\n",
    "\n",
    "THE CODE BELOW MERGES ALL THE CSV FILES FOR 4th TIMESTAMP (RIGHT BEFORE NEEDLE INJECTION) INTO ONE DATAFRAME AND SAVES IT IN NEW CSV FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8499537a-1a72-452f-b872-c01d2f54b6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path =  \"confidential\"\n",
    "\n",
    "num_files = len(glob.glob(os.path.join(folder_path, \"*4_final_imputation.csv\")))\n",
    "print(num_files)\n",
    "# New list for new dataframes\n",
    "dfs = []\n",
    "\n",
    "# Loop for files from 4th timepoint\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\"4_final_imputation.csv\"):\n",
    "        # Read the CSV file and append it to the list\n",
    "        df = pd.read_csv(os.path.join(folder_path, filename))\n",
    "        dfs.append(df)\n",
    "\n",
    "# Merging dataframes in one large one\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "merged_df.head\n",
    "# Saving dataframe to new csv file\n",
    "merged_df.to_csv(os.path.join(folder_path, \"Timepoint_4_merged.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33f2326-cc1c-41cc-8efb-35dd7724135d",
   "metadata": {},
   "source": [
    "THE CODE BELOW EXTRACTS SUM OF POINTS FROM VVR QUESTIONNAIRE FILE AT TIMEPOINT 4 AND CREATES BINARY CLASSIFICATION COLUMN 'At_Risk', which returns 1 when patient \n",
    "is At_Risk (score >11 out of 40) and 1 when patient is not At_Risk (score<11 out of 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7c97d6-470c-4618-873b-12c98d73458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(\"Timepoint_4_merged.csv\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('20221004_Time_points_final.csv', sep = \";\", nrows=1989, encoding='latin1')\n",
    "\n",
    "df = df[df['Time_point'] == 4]\n",
    "df = df[['sum', 'ID']]\n",
    "\n",
    "\n",
    "merged_df = pd.merge(merged_df, df, on='ID', how='left')\n",
    "merged_df ['sum'] = merged_df ['sum'].fillna(0)  \n",
    "merged_df ['At_Risk'] = merged_df ['sum'].apply(lambda x: 1 if x > 11 else 0)\n",
    "merged_df = merged_df [['ID', 'sum', 'At_Risk'] + [col for col in merged_df .columns if col not in ['ID', 'sum', 'At_Risk']]]\n",
    "merged_df = merged_df.reset_index(drop=True) \n",
    "\n",
    "\n",
    "final = merged_df[['ID', 'At_Risk']].copy()\n",
    "\n",
    "\n",
    "y = final.groupby('ID').first().reset_index()\n",
    "y.set_index('ID', inplace=True)\n",
    "y = y['At_Risk'].squeeze()\n",
    "\n",
    "# Drop unecessary columns:\n",
    "merged_df = merged_df.drop(columns=['sum','At_Risk'])\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036257f4-8bdc-4142-8d46-77123dd36979",
   "metadata": {},
   "source": [
    "FEATURE EXTRACTION PROCEDURE FROM AUs DATAFRAME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1e75eb-aa7a-4d07-9c6c-6b7a912b8a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "\n",
    "A = extract_features(merged_df, column_id='ID', column_sort='Frame', default_fc_parameters=ComprehensiveFCParameters(),impute_function = impute)\n",
    "                    \n",
    "A =impute(A)\n",
    "rows_with_nan = A[A.isna().any(axis=1)]\n",
    "\n",
    "#Creating Final Dataframe With Features\n",
    "X_final_A = A.dropna(subset=rows_with_nan.columns)\n",
    "X_final_A= pd.DataFrame(X_final_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1481a072-8646-443b-b451-f3415e1b9d72",
   "metadata": {},
   "source": [
    "Selecting Relevant Features using select_features function in tsfresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04de6d77-df2e-4f4f-9236-c6c8f9b8f0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filtered = select_features(X_final_A, y,ml_task = 'classification', fdr_level=0.275, hypotheses_independent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4814939-4e63-4560-970c-3750080e0b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "Machine Learning Process Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4335fad7-800d-4069-8786-fbba398e5385",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = X_filtered\n",
    "y = y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Splitting data into training, validation, and test sets\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.2, random_state=42, stratify=y_trainval)\n",
    "\n",
    "# Standardizing data\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_val_std = scaler.transform(X_val)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Applying SMOTE on Train datasets\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_std, y_train)\n",
    "\n",
    "#Setting Scoring for GridSearchCV for recall, as we want recall as the most important scoring method. \n",
    "scoring = make_scorer(recall_score)\n",
    "\n",
    "\n",
    "# Random Forest Classifier with grid search cross-validation\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "param_grid_rf = {'n_estimators': [50, 100, 200,500],\n",
    "                 'max_depth': [5, 10, 20, None],\n",
    "                 'bootstrap': [True, False]}\n",
    "rf_gs = GridSearchCV(rf, param_grid_rf, cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42), scoring=scoring)\n",
    "rf_gs.fit(X_train_smote, y_train_smote)\n",
    "print(\"Random Forest Classifier Best Parameters: \", rf_gs.best_params_)\n",
    "\n",
    "# Non-Linear SVM Classifier with grid search cross-validation\n",
    "svm = SVC(kernel='rbf', random_state=42)\n",
    "param_grid_svm = {'C': [0.1, 1, 10],\n",
    "        'gamma': ['scale', 'auto', 0.1, 1, 10],\n",
    "        'coef0': [0.0, 0.5, 1.0],\n",
    "        'degree': [2, 3, 4]}\n",
    "svm_gs = GridSearchCV(svm, param_grid_svm, cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42), scoring=scoring)\n",
    "svm_gs.fit(X_train_smote, y_train_smote)\n",
    "print(\"Non-Linear SVM Classifier Best Parameters: \", svm_gs.best_params_)\n",
    "\n",
    "\n",
    "# LightGBM Classifier with grid search cross-validation\n",
    "lgbm_clf = lgbm.LGBMClassifier(random_state=42)\n",
    "param_grid_lgbm = {'n_estimators': [50, 100, 200],\n",
    "                   'max_depth': [5, 10, 20, None],\n",
    "                   'learning_rate': [0.01, 0.1, 0.5],\n",
    "                   'num_leaves': [31, 50, 100, 200]}\n",
    "lgbm_gs = GridSearchCV(lgbm_clf, param_grid_lgbm, cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42), scoring=scoring)\n",
    "lgbm_gs.fit(X_train_smote, y_train_smote)\n",
    "print(\"LightGBM Classifier Best Parameters: \", lgbm_gs.best_params_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=rf_gs.best_params_['n_estimators'], max_depth=rf_gs.best_params_['max_depth'],\n",
    "                             bootstrap =rf_gs.best_params_['bootstrap'], random_state=42)\n",
    "rf_clf.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Non-Linear SVM Classifier\n",
    "svm_clf = SVC(C=svm_gs.best_params_['C'], gamma=svm_gs.best_params_['gamma'],coef0=svm_gs.best_params_['coef0'], degree=svm_gs.best_params_['degree'],\n",
    "              kernel='rbf', random_state=42)\n",
    "svm_clf.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "\n",
    "# LightGBM Classifier\n",
    "lgbm_clf = lgbm.LGBMClassifier(n_estimators=lgbm_gs.best_params_['n_estimators'], max_depth=lgbm_gs.best_params_['max_depth'], \n",
    "                               learning_rate =lgbm_gs.best_params_['learning_rate'], num_leaves =lgbm_gs.best_params_['num_leaves'],   random_state=42)\n",
    "lgbm_clf.fit(X_train_smote, y_train_smote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75d9457-d428-4db9-af48-b33c490988ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING SET EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f87dc0-5d65-4cb6-bcc4-4ae3658a8999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on train set\n",
    "y_pred_rf = rf_clf.predict(X_train_std)\n",
    "y_pred_svm = svm_clf.predict(X_train_std)\n",
    "y_pred_lgbm = lgbm_clf.predict(X_train_std)\n",
    "y_pred_linear_svm = linear_svm_gs.predict(X_train_std)\n",
    "\n",
    "# Performance Metrics for Train Set\n",
    "print(\"Random Forest Classifier Precision on Training Data: \", precision_score(y_train, y_pred_rf))\n",
    "print(\"Random Forest Classifier Recall on Training Data: \", recall_score(y_train, y_pred_rf))\n",
    "print(\"Random Forest Classifier F1-score on Training Data: \", f1_score(y_train, y_pred_rf))\n",
    "print(\"Random Forest Classifier ROC AUC on Training Data: \", roc_auc_score(y_train, rf_clf.predict_proba(X_train_std)[:, 1]))\n",
    "print(\"Random Forest Classifier PR AUC on Training Data: \", average_precision_score(y_train, rf_clf.predict_proba(X_train_std)[:, 1]))\n",
    "print(\"Random Forest Classifier Matthews Coefficient on Training Data: \", matthews_corrcoef(y_train, y_pred_rf))\n",
    "\n",
    "print(\"Non-Linear SVM Classifier Precision on Training Data: \", precision_score(y_train, y_pred_svm))\n",
    "print(\"Non-Linear SVM Classifier Recall on Training Data: \", recall_score(y_train, y_pred_svm))\n",
    "print(\"Non-Linear SVM Classifier F1-score on Training Data: \", f1_score(y_train, y_pred_svm))\n",
    "print(\"Non-Linear SVM Classifier ROC AUC on Training Data: \", roc_auc_score(y_train, svm_clf.decision_function(X_train_std)))\n",
    "print(\"Non-Linear SVM Classifier PR AUC on Training Data: \", average_precision_score(y_train, svm_clf.decision_function(X_train_std)))\n",
    "print(\"Non-Linear SVM Classifier Matthews Coefficient on Training Data: \", matthews_corrcoef(y_train, y_pred_svm))\n",
    "\n",
    "print(\"Linear SVM Classifier Precision on Training Data: \", precision_score(y_train, y_pred_linear_svm))\n",
    "print(\"Linear SVM Classifier Recall on Training Data: \", recall_score(y_train, y_pred_linear_svm))\n",
    "print(\"Linear SVM Classifier F1-score on Training Data: \", f1_score(y_train, y_pred_linear_svm))\n",
    "print(\"Linear SVM Classifier ROC AUC on Training Data: \", roc_auc_score(y_train, linear_svm_clf.decision_function(X_train_std)))\n",
    "print(\"Linear SVM Classifier PR AUC on Training Data: \", average_precision_score(y_train, linear_svm_clf.decision_function(X_train_std)))\n",
    "print(\"Linear SVM Classifier Matthews Coefficient on Training Data: \", matthews_corrcoef(y_train, y_pred_linear_svm))\n",
    "\n",
    "print(\"LightGBM Classifier Precision on Training Data: \", precision_score(y_train, y_pred_lgbm))\n",
    "print(\"LightGBM Classifier Recall on Training Data: \", recall_score(y_train, y_pred_lgbm))\n",
    "print(\"LightGBM Classifier F1-score on Training Data: \", f1_score(y_train, y_pred_lgbm))\n",
    "print(\"LightGBM Classifier ROC AUC on Training Data: \", roc_auc_score(y_train, lgbm_clf.predict_proba(X_train_std)[:, 1]))\n",
    "print(\"LightGBM Classifier PR AUC on Training Data: \", average_precision_score(y_train, lgbm_clf.predict_proba(X_train_std)[:, 1]))\n",
    "print(\"LightGBM Classifier Matthews Coefficient on Training Data: \", matthews_corrcoef(y_train, y_pred_lgbm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba74221c-9b39-4fd7-b738-fb00d2ddaf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION SET EVALAUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f5d8d8-4b42-4ad4-b799-d99941b69249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on validation set\n",
    "y_pred_rf = rf_clf.predict(X_val_std)\n",
    "y_pred_svm = svm_clf.predict(X_val_std)\n",
    "y_pred_lgbm = lgbm_clf.predict(X_val_std)\n",
    "y_pred_linear_svm = linear_svm_gs.predict(X_val_std)\n",
    "\n",
    "# Performance Metrics of Validation Sets\n",
    "print(\"Random Forest Classifier Precision on Validation Data: \", precision_score(y_val, y_pred_rf))\n",
    "print(\"Random Forest Classifier Recall on Validation Data: \", recall_score(y_val, y_pred_rf))\n",
    "print(\"Random Forest Classifier F1-score on Validation Data: \", f1_score(y_val, y_pred_rf))\n",
    "print(\"Random Forest Classifier ROC AUC on Validation Data: \", roc_auc_score(y_val, rf_clf.predict_proba(X_val_std)[:, 1]))\n",
    "print(\"Random Forest Classifier PR AUC on Validation Data: \", average_precision_score(y_val, rf_clf.predict_proba(X_val_std)[:, 1]))\n",
    "print(\"Random Forest Classifier Matthews Coefficient on Validation Data: \", matthews_corrcoef(y_val, y_pred_rf))\n",
    "\n",
    "print(\"Non-Linear SVM Classifier Precision on Validation Data: \", precision_score(y_val, y_pred_svm))\n",
    "print(\"Non-Linear SVM Classifier Recall on Validation Data: \", recall_score(y_val, y_pred_svm))\n",
    "print(\"Non-Linear SVM Classifier F1-score on Validation Data: \", f1_score(y_val, y_pred_svm))\n",
    "print(\"Non-Linear SVM Classifier ROC AUC on Validation Data: \", roc_auc_score(y_val, svm_clf.decision_function(X_val_std)))\n",
    "print(\"Non-Linear SVM Classifier PR AUC on Validation Data: \", average_precision_score(y_val, svm_clf.decision_function(X_val_std)))\n",
    "print(\"Non-Linear SVM Classifier Matthews Coefficient on Validation Data: \", matthews_corrcoef(y_val, y_pred_svm))\n",
    "\n",
    "print(\"Linear SVM Classifier Precision on Validation Data: \", precision_score(y_val, y_pred_linear_svm))\n",
    "print(\"Linear SVM Classifier Recall on Validation Data: \", recall_score(y_val, y_pred_linear_svm))\n",
    "print(\"Linear SVM Classifier F1-score on Validation Data: \", f1_score(y_val, y_pred_linear_svm))\n",
    "print(\"Linear SVM Classifier ROC AUC on Validation Data: \", roc_auc_score(y_val, linear_svm_clf.decision_function(X_val_std)))\n",
    "print(\"Linear SVM Classifier PR AUC on Validation Data: \", average_precision_score(y_val, linear_svm_clf.decision_function(X_val_std)))\n",
    "print(\"Linear SVM Classifier Matthews Coefficient on Validation Data: \", matthews_corrcoef(y_val, y_pred_linear_svm))\n",
    "\n",
    "print(\"LightGBM Classifier Precision on Validation Data: \", precision_score(y_val, y_pred_lgbm))\n",
    "print(\"LightGBM Classifier Recall on Validation Data: \", recall_score(y_val, y_pred_lgbm))\n",
    "print(\"LightGBM Classifier F1-score on Validation Data: \", f1_score(y_val, y_pred_lgbm))\n",
    "print(\"LightGBM Classifier ROC AUC on Validation Data: \", roc_auc_score(y_val, lgbm_clf.predict_proba(X_val_std)[:, 1]))\n",
    "print(\"LightGBM Classifier PR AUC on Validation Data: \", average_precision_score(y_val, lgbm_clf.predict_proba(X_val_std)[:, 1]))\n",
    "print(\"LightGBM Classifier Matthews Coefficient on Validation Data: \", matthews_corrcoef(y_val, y_pred_lgbm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98851aa2-2897-4605-ac3b-d7455f5b94ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST SET EVALAUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f80ea81-3b4d-4883-a1d3-3e4c7ab7bc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Making predictions on validation data using best hyperparameters\n",
    "y_pred_rf = rf_clf.predict(X_test_std)\n",
    "y_pred_svm = svm_clf.predict(X_test_std)\n",
    "y_pred_lgbm = lgbm_clf.predict(X_test_std)\n",
    "y_pred_linear_svm = linear_svm_gs.predict(X_test_std)\n",
    "# Evaluating performance on validation data\n",
    "print(\"Random Forest Classifier Precision on Validation Data: \", precision_score(y_test, y_pred_rf))\n",
    "print(\"Random Forest Classifier Recall on Validation Data: \", recall_score(y_test, y_pred_rf))\n",
    "print(\"Random Forest Classifier F1-score on Validation Data: \", f1_score(y_test, y_pred_rf))\n",
    "print(\"Random Forest Classifier ROC AUC on Validation Data: \", roc_auc_score(y_test, rf_clf.predict_proba(X_test_std)[:, 1]))\n",
    "print(\"Random Forest Classifier PR AUC on Validation Data: \", average_precision_score(y_test, rf_clf.predict_proba(X_test_std)[:, 1]))\n",
    "print(\"Random Forest Classifier Matthews Coefficient on Validation Data: \", matthews_corrcoef(y_test, y_pred_rf))\n",
    "\n",
    "print(\"Non-Linear SVM Classifier Precision on Validation Data: \", precision_score(y_test, y_pred_svm))\n",
    "print(\"Non-Linear SVM Classifier Recall on Validation Data: \", recall_score(y_test, y_pred_svm))\n",
    "print(\"Non-Linear SVM Classifier F1-score on Validation Data: \", f1_score(y_test, y_pred_svm))\n",
    "print(\"Non-Linear SVM Classifier ROC AUC on Validation Data: \", roc_auc_score(y_test, svm_clf.decision_function(X_test_std)))\n",
    "print(\"Non-Linear SVM Classifier PR AUC on Validation Data: \", average_precision_score(y_test, svm_clf.decision_function(X_test_std)))\n",
    "print(\"Non-Linear SVM Classifier Matthews Coefficient on Validation Data: \", matthews_corrcoef(y_test, y_pred_svm))\n",
    "\n",
    "print(\"Linear SVM Classifier Precision on Validation Data: \", precision_score(y_test, y_pred_linear_svm))\n",
    "print(\"Linear SVM Classifier Recall on Validation Data: \", recall_score(y_test, y_pred_linear_svm))\n",
    "print(\"Linear SVM Classifier F1-score on Validation Data: \", f1_score(y_test, y_pred_linear_svm))\n",
    "print(\"Linear SVM Classifier ROC AUC on Validation Data: \", roc_auc_score(y_test, linear_svm_clf.decision_function(X_test_std)))\n",
    "print(\"Linear SVM Classifier PR AUC on Validation Data: \", average_precision_score(y_test, linear_svm_clf.decision_function(X_test_std)))\n",
    "print(\"Linear SVM Classifier Matthews Coefficient on Validation Data: \", matthews_corrcoef(y_test, y_pred_linear_svm))\n",
    "\n",
    "print(\"LightGBM Classifier Precision on Validation Data: \", precision_score(y_test, y_pred_lgbm))\n",
    "print(\"LightGBM Classifier Recall on Validation Data: \", recall_score(y_test, y_pred_lgbm))\n",
    "print(\"LightGBM Classifier F1-score on Validation Data: \", f1_score(y_test, y_pred_lgbm))\n",
    "print(\"LightGBM Classifier ROC AUC on Validation Data: \", roc_auc_score(y_test, lgbm_clf.predict_proba(X_test_std)[:, 1]))\n",
    "print(\"LightGBM Classifier PR AUC on Validation Data: \", average_precision_score(y_test, lgbm_clf.predict_proba(X_test_std)[:, 1]))\n",
    "print(\"LightGBM Classifier Matthews Coefficient on Validation Data: \", matthews_corrcoef(y_test, y_pred_lgbm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bc3f7b-1e71-4069-b644-93d59dedade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATING IMPORTANCE SCORES FOR FEATURES PER TIMEPOINT FOR EACH MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e5d4b1-865b-4152-b541-f0c0fd4f568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANDOM FOREST\n",
    "model_rf = RandomForestClassifier(bootstrap = True, max_depth = 10, n_estimators = 100)\n",
    "model_rf.fit(X_filtered, y)\n",
    "\n",
    "sorted_indices_rf = importance_scores_rf.argsort()[::-1]\n",
    "sorted_feature_names_rf = df_rf['Feature'].iloc[sorted_indices_rf]\n",
    "sorted_importance_scores_rf = importance_scores_rf[sorted_indices_rf]\n",
    "\n",
    "# Dataframe with name and importance score of the feature\n",
    "df_sorted_rf = pd.DataFrame({'Feature': sorted_feature_names_rf, 'Importance Score': sorted_importance_scores_rf})\n",
    "df_sorted_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb278be5-e195-4a6a-b349-5c8965651c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non-linear SVM\n",
    "model_svm = SVC(kernel='rbf', C=10, coef0=0.0, degree=2, gamma='scale')  # Non-linear SVM with RBF kernel\n",
    "model_svm.fit(X_filtered, y)\n",
    "\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "result = permutation_importance(model_svm, X_filtered, y, n_repeats=10, random_state=42)\n",
    "\n",
    "\n",
    "importance_scores_svm = result.importances_mean\n",
    "\n",
    "sorted_indices_svm = importance_scores_svm.argsort()[::-1]\n",
    "sorted_feature_names_svm = X_filtered.columns[sorted_indices_svm]\n",
    "sorted_importance_scores_svm = importance_scores_svm[sorted_indices_svm]\n",
    "\n",
    "# Dataframe with name and importance score of the feature\n",
    "df_sorted_svm = pd.DataFrame({'Feature': sorted_feature_names_svm, 'Importance Score': sorted_importance_scores_svm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b974b6-5535-4f53-b6f5-13fed6c52359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LightGBM\n",
    "model_lgb = lgb.LGBMClassifier(learning_rate=0.5, max_depth=10, n_estimators=100, num_leaves=31)\n",
    "model_lgb.fit(X_filtered, y)\n",
    "\n",
    "importance_scores_lgb = model_lgb.feature_importances_\n",
    "normalized_importance_scores_lgb = importance_scores_lgb / importance_scores_lgb.sum()\n",
    "\n",
    "sorted_indices_lgb = normalized_importance_scores_lgb.argsort()[::-1]\n",
    "sorted_feature_names_lgb = X_filtered.columns[sorted_indices_lgb]\n",
    "sorted_importance_scores_lgb = normalized_importance_scores_lgb[sorted_indices_lgb]\n",
    "\n",
    "# Dataframe with name and importance score of the feature\n",
    "df_sorted_lgb = pd.DataFrame({'Feature': sorted_feature_names_lgb, 'Importance Score': sorted_importance_scores_lgb})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe5a061-ab23-4cb7-9a8e-5e68762f90db",
   "metadata": {},
   "source": [
    "ERROR ANALYSIS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f2613b-1dcc-4ed2-ba86-95624a5d3667",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "self_reported_df.drop_duplicates(subset='ID', inplace=True)\n",
    "\n",
    "# Define the threshold for the binary classification\n",
    "threshold = 11\n",
    "\n",
    "# Convert 'X_test_std' array to a DataFrame\n",
    "X_test_std = pd.DataFrame(X_test_std, index=y_test.index)\n",
    "\n",
    "predictions_rf = rf_clf.predict(X_test_std)\n",
    "predictions_non_linear_svm = svm_clf.predict(X_test_std)\n",
    "predictions_lgbm = lgbm_clf.predict(X_test_std)\n",
    "\n",
    "prediction_variables = ['predictions_rf', 'predictions_non_linear_svm', 'predictions_lgbm']\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4)) \n",
    "\n",
    "# Creating three bar charts \n",
    "for i, variable in enumerate(prediction_variables):\n",
    "    \n",
    "    predictions = globals()[variable]\n",
    "    plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    predictions_series = pd.Series(predictions, index=y_test.index, name='Prediction')\n",
    "\n",
    "    merged_df = pd.merge(self_reported_df, predictions_series, left_on='ID', right_index=True, how='inner')\n",
    "\n",
    "    # Calculate the counts of correct and incorrect predictions for each VVR score\n",
    "    vvr_scores = merged_df['sum'].unique()\n",
    "    correct_counts = []\n",
    "    incorrect_counts = []\n",
    "\n",
    "    for vvr_score in vvr_scores:\n",
    "        df_subset = merged_df[merged_df['sum'] == vvr_score]\n",
    "        correct_count = len(df_subset[df_subset['Prediction'] == df_subset['At_Risk']])\n",
    "        incorrect_count = len(df_subset) - correct_count\n",
    "        correct_counts.append(correct_count)\n",
    "        incorrect_counts.append(incorrect_count)\n",
    "\n",
    "    # Stacked Bar charts with correct predictions\n",
    "    axs[i].bar(vvr_scores, correct_counts, color='gray', label='Correct Predictions', hatch='')\n",
    "\n",
    "    # Stacked Bar charts with incorrect predictions on top of correct predictions\n",
    "    axs[i].bar(vvr_scores, incorrect_counts, bottom=correct_counts, color='white', edgecolor='black',\n",
    "               label='Incorrect Predictions', hatch='////')\n",
    "\n",
    "    # Threshold line of 11\n",
    "    axs[i].axvline(x=11, color='black', linestyle='dotted')\n",
    "    \n",
    "    # Titles for bar charts\n",
    "    if variable == 'predictions_rf':\n",
    "        title = 'Random Forest'\n",
    "    elif variable == 'predictions_non_linear_svm':\n",
    "        title = 'Non-linear SVM'\n",
    "    elif variable == 'predictions_lgbm':\n",
    "        title = 'LightGBM'\n",
    "\n",
    "    axs[i].set_xlabel('VVR SCORES')\n",
    "    axs[i].set_ylabel('Number of Donors')\n",
    "    axs[i].set_title(f'{title}')\n",
    "    axs[i].set_ylim(top=15)\n",
    "    axs[i].legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    " \n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "# Save the plot as a JPEG image\n",
    "plt.savefig('prediction_plot_timepoint4_test_set.jpeg', format='jpeg')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
